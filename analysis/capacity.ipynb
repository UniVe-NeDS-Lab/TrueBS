{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_632583/3090532007.py:7: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('pdf', 'svg')\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "%matplotlib inline\n",
    "# produce vector inline graphics\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import rasterio as rio\n",
    "import rasterio.mask\n",
    "import csv\n",
    "import shapely.wkt as wkt\n",
    "import math as m\n",
    "from scipy.constants import c as speed_of_light\n",
    "import random\n",
    "\n",
    "def conf_int(data, axis=0, alpha=0.95, weights=None):\n",
    "    data = np.array(data)\n",
    "    if not weights:\n",
    "        return scipy.stats.sem(data, axis=axis) * scipy.stats.t.ppf((1 + alpha) / 2., data.shape[axis]-1)\n",
    "    else:\n",
    "        mean = np.average(data, axis=axis, weights=weights)\n",
    "        stdev = np.sqrt(np.average((data-mean)**2, weights=weights, axis=axis))\n",
    "        sem = stdev/np.sqrt(data.shape[axis])\n",
    "        return sem * scipy.stats.t.ppf((1 + alpha) / 2., data.shape[axis]-1)\n",
    "\n",
    "def std(data, axis=0, weights=None):\n",
    "    data = np.array(data)\n",
    "    if not weights:\n",
    "        return np.std(data, axis=axis)\n",
    "    else:\n",
    "        mean = np.average(data, axis=axis, weights=weights)\n",
    "        stdev = np.sqrt(np.average((data-mean)**2, weights=weights, axis=axis))\n",
    "        return stdev\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#PARAMS\n",
    "area = 'luxemburg'\n",
    "sub_area_id = '0'\n",
    "max_dist =  300\n",
    "\n",
    "#RF params\n",
    "h_bs = 10\n",
    "h_ut = 1.5\n",
    "epsg = 3003\n",
    "kb = 1.380649*(10**-23)  # J/K\n",
    "temp = 300  # K\n",
    "nf = 5\n",
    "fronthaul_bandwidth = 400e6 #Hz\n",
    "fronthaul_mimo = 2\n",
    "frequency = 28 #Ghz\n",
    "tx_power = 30\n",
    "tx_gain = 10\n",
    "rx_gain_fronthaul = 3  # need to find a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small area (2)\n",
    "sub_area = wkt.loads(\"POLYGON ((76149.3828127647 73922.4335140161,76110.5488176897 74065.2871387564,75745.7866496635 74733.7866254053,75974.6298349271 74830.8716130929,76478.084842507 75002.8507341395,76532.175049933 74790.6506896223,76514.144980791 74653.3447784641,76514.144980791 74529.9081512613,76584.8783289634 74276.100254878,76451.7332029918 73934.9158695759,76149.3828127647 73922.4335140161))\")\n",
    "#big area (0)\n",
    "sub_area = wkt.loads(\"MULTIPOLYGON (((77462.8497438607 73703.6291953883,75345.6534984474 73935.3621177719,75366.720127755 74009.0953203485,74843.5654999498 74153.050620617,74836.5432901806 74184.6505645784,74889.2098634495 74542.7832628075,74854.0988146036 74985.182478267,74861.1210243728 75301.1819178809,74882.1876536804 75536.4259451491,74959.4319611415 75813.8032310324,76511.3403201343 75448.6483230341,76967.7839551322 75329.2707569577,76985.3394795552 74995.7157929208,77564.671785514 74005.5842154639,77462.8497438607 73703.6291953883)))\")\n",
    "\n",
    "\n",
    "folders = glob.glob('/mnt/ric_dais_nfs_maccari/gabriel/results/ComComVSI/*')\n",
    "targets = {}\n",
    "transforms_tgt = {}\n",
    "for f in folders:\n",
    "    t = f.split(sep='/')[-1][8:]\n",
    "    dataset = rio.open(f'/mnt/ric_dais_nfs_maccari/gabriel/data/dtm_fusion/luxemburg_{t}_mask.tif', 'r', crs='EPSG:2169')\n",
    "    raster, transform =  rio.mask.mask(dataset, [sub_area], crop=True, indexes=1)    \n",
    "    targets[t] = raster\n",
    "    transforms_tgt[t] = transform\n",
    "\n",
    "dtm_dataset = rio.open(f'/mnt/ric_dais_nfs_maccari/gabriel/data/dtm_fusion/{area}_dtm.tif')\n",
    "with open(f'../areas/{area}.csv') as sacsv:\n",
    "    subareas_csv = list(csv.reader(sacsv, delimiter=','))\n",
    "    for sa in subareas_csv:\n",
    "        if sa[1].strip() == sub_area_id:\n",
    "            sub_area = wkt.loads(sa[0])\n",
    "            buffered_area = sub_area.buffer(max_dist/2)\n",
    "            dataset_crop, rm_transform = rio.mask.mask(dtm_dataset, [buffered_area], crop=True, indexes=1)\n",
    "            dataset_unb_crop, rm_unb_transform = rio.mask.mask(dtm_dataset, [sub_area], crop=True, indexes=1)\n",
    "\n",
    "\n",
    "def mytransf(points, transform1, transform2):\n",
    "    rows, cols = list(zip(*points))\n",
    "    newrows, newcols = rio.transform.rowcol(transform1, *rio.transform.xy(transform2, rows, cols))\n",
    "    return list(zip(newrows, newcols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import vectorize, float64, uint8\n",
    "\n",
    "@vectorize([float64(float64, uint8)])\n",
    "def pathloss(d, los):\n",
    "    # ETSI TR38.901 Channel Model\n",
    "    if d < 10: # Model is undefined for d<10\n",
    "        d = 10  \n",
    "    breakpoint_distance = 2*m.pi*h_bs*h_ut*frequency*1e9/speed_of_light\n",
    "    if d < breakpoint_distance:\n",
    "        pl_los = 32.4 + 21*m.log10(d)+20*m.log10(frequency)  # + nrv_los.rvs(1)[0]\n",
    "    else:\n",
    "        pl_los = 32.4 + 40*m.log10(d)+20*m.log10(frequency) - 9.5*m.log10((breakpoint_distance)**2 + (h_bs-h_ut)**2)  # + nrv_los.rvs(1)[0]\n",
    "\n",
    "    pl_nlos = 22.4 + 35.3*m.log10(d)+21.3*m.log10(frequency) - 0.3*(h_ut - 1.5)  # + nrv_nlos.rvs(1)[0]\n",
    "\n",
    "    if los:\n",
    "        return pl_los\n",
    "    else:\n",
    "        return max(pl_los, pl_nlos)\n",
    "\n",
    "def get_shannon_capacity(los, d):\n",
    "    loss = tx_power + tx_gain - pathloss(d, los) + rx_gain_fronthaul\n",
    "    noise = 10*m.log10(1000*kb*temp*fronthaul_bandwidth)\n",
    "    snr =  loss - noise  - nf\n",
    "    lin_snr = 10**(snr/10)\n",
    "    bw = fronthaul_mimo*fronthaul_bandwidth*np.log2(1+lin_snr) * 10**-6 #to get Mbps\n",
    "    return bw\n",
    "\n",
    "def get_max_capacity(los, dist):\n",
    "    #compute shannon capacity on the matrix\n",
    "    capacities = get_shannon_capacity(los, dist)\n",
    "    pl = pathloss(dist, los)\n",
    "    #set capacity 0 to links longer than max_dist\n",
    "    capacities = np.where(dist>max_dist, 0, capacities)\n",
    "    \n",
    "    #get bs_ids that correspond to max capacity for each point (over bs axis)\n",
    "    bs_i = np.expand_dims(np.argmax(capacities, axis=0), axis=0)\n",
    "    #get maximum capacities\n",
    "    max_c = np.take_along_axis(capacities, bs_i, axis=0)[0]\n",
    "    #get pathloss of those points\n",
    "    min_pl = np.take_along_axis(pl, bs_i, axis=0)[0]\n",
    "    #get dist of those points\n",
    "    max_dists = np.take_along_axis(dist, bs_i, axis=0)[0]\n",
    "    #get visibility of those points\n",
    "    los_chosen = np.take_along_axis(los, bs_i, axis=0)[0]\n",
    "    #reverse to get non visibiltiy\n",
    "    nlos_chosen = np.where(los_chosen==1, 0, 1)\n",
    "    #get capacity for los link\n",
    "    capacities_los = np.multiply(max_c, los_chosen)\n",
    "    #get capacity for nlos link\n",
    "    capacities_nlos = np.multiply(max_c, nlos_chosen)\n",
    "    #get links that i chose to be nlos even if there was a los (capacity nlos>los)\n",
    "    p_los = np.bitwise_or.reduce(los, axis=0)\n",
    "    tr_nlos = np.bitwise_xor(p_los, los_chosen)\n",
    "    # print(p_los.shape)\n",
    "    # print(tr_nlos.shape)\n",
    "    return max_c, capacities_los, capacities_nlos, max_dists, los_chosen, min_pl, tr_nlos, p_los"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate figure for shannon capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(plos):\n",
    "    loss = tx_power + tx_gain - plos + rx_gain_fronthaul\n",
    "    noise = 10*m.log10(1000*kb*temp*fronthaul_bandwidth)\n",
    "    snr =  loss - noise  - nf\n",
    "    lin_snr = 10**(snr/10)\n",
    "    bw = fronthaul_mimo*fronthaul_bandwidth*np.log2(1+lin_snr) * 10**-9\n",
    "    return np.array([loss, snr, bw])\n",
    "\n",
    "x = np.linspace(0,300, 1000)\n",
    "pl_los = plot(pathloss(x, 1))\n",
    "pl_nlos = plot(pathloss(x, 0))\n",
    "\n",
    "\n",
    "data = np.zeros(shape=(3, len(x)))\n",
    "data[0,: ] = x\n",
    "data[1, :] = pl_los[2, :]\n",
    "data[2, :] = pl_nlos[2, :]\n",
    "\n",
    "np.savetxt(\"../processed_results/shannon.csv\", data.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "#warnings.filters()\n",
    "def cuda_los(viewsheds, np_points, nodes):\n",
    "    los=cuda.device_array(shape=(len(nodes), len(np_points)), dtype=np.uint8)\n",
    "    threadsperblock = (16, 16)\n",
    "    blockspergrid_x = m.ceil(los.shape[0] / threadsperblock[0])\n",
    "    blockspergrid_y = m.ceil(los.shape[1] / threadsperblock[1])\n",
    "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "    gen_los[blockspergrid, threadsperblock](los, viewsheds, np_points)\n",
    "    return los.copy_to_host()\n",
    "\n",
    "@cuda.jit()\n",
    "def gen_los(los, viewsheds, np_points):\n",
    "    i, j = cuda.grid(2)\n",
    "    if i<los.shape[0] and j<los.shape[1]:\n",
    "        los[i,j] = viewsheds[i,  np_points[j, 0], np_points[j,1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = glob.glob(f'/mnt/ric_dais_nfs_maccari/gabriel/results/ComComVSI/*/{area}/threestep/{sub_area_id}/r1/1/100.0/*/')\n",
    "res = []\n",
    "res_un = []\n",
    "for r in runs[:]:\n",
    "    #print(r)\n",
    "    prefix, subpath = r.split('results_')\n",
    "    params =  subpath.split('/')\n",
    "    base, area, strat, sa, rf, k, ratio, dens = params[0:-1]\n",
    "    #print(base, dens)\n",
    "    if base not in ['sidewalkparks', 'cars_sumo_nonorm', 'cars_sumo']:\n",
    "        continue\n",
    "    bp = prefix+'results_'+'/'.join(params[:4])\n",
    "    common_d = {'area': area, 'base': base, 'dens': int(dens)}\n",
    "    try:\n",
    "        nodes = pd.read_csv(f'{r}/index.csv', \n",
    "                            sep=' ', \n",
    "                            header=None, \n",
    "                            names=['x', 'y', 'z', 'x_3003', 'y_3003', 'b', 'p_i']\n",
    "                            )#.set_index('p_i')\n",
    "        viewsheds = np.load(f'{r}/viewsheds.npy')\n",
    "    except:\n",
    "        continue\n",
    "    for tgt in ['sidewalkparks', 'cars_sumo_nonorm', 'cars_sumo']:\n",
    "        #Get list of points from the target points matrix\n",
    "        points = np.argwhere(targets[tgt])\n",
    "        #Extract a random subset of points\n",
    "        rand_points = random.choices(points, k=len(points)//10)\n",
    "        #Reproject from unbuffered to buffered area\n",
    "        t_points = mytransf(rand_points, rm_transform, transforms_tgt[tgt])\n",
    "        #Get base points\n",
    "        base_points = []\n",
    "        #convert nodes to a 3d numpy array and add u_ht as z value\n",
    "        nodes_np = np.zeros(shape=(nodes.shape[0], 3), dtype=np.float32)\n",
    "        nodes_np[:, :] = nodes[[\"x\", \"y\", \"z\"]].to_numpy()\n",
    "        #nodes_np[:, 2] = h_ut\n",
    "        \n",
    "        #generate set of points to check \n",
    "        np_points = np.zeros(shape=(len(t_points), 3), dtype=np.uint)\n",
    "        np_points[:, :2] = np.array(t_points)\n",
    "        #get height from dtm\n",
    "        for p_i, pt in enumerate(np_points):\n",
    "            np_points[p_i, 2] = dataset_crop[pt[0], pt[1]]\n",
    "        \n",
    "        #use cuda to generate los matrix\n",
    "        los = cuda_los(viewsheds, np_points, nodes_np)\n",
    "        #do some magic with numpy to get a matrix of distances between BS and points\n",
    "        dist = np.fromfunction(lambda i,j: np.linalg.norm(nodes_np[i]-np_points[j], axis=2),\n",
    "                              shape=(len(nodes), len(t_points)),\n",
    "                              dtype=int)\n",
    "        #compute the maximum capacity on the whole matrix\n",
    "        mc = get_max_capacity(los, dist)\n",
    "        #save the capacities with the normal factor in their appropriate array\n",
    "        base_points = []\n",
    "        speed = []\n",
    "        base_points_los = []\n",
    "        speed_los = []\n",
    "        base_points_nlos = []\n",
    "        speed_nlos = []\n",
    "        tr_nlos = []\n",
    "        los = []\n",
    "\n",
    "        for p_i, p_t in enumerate(t_points):\n",
    "            res_un.append({'target': tgt, 'speed': mc[0][p_i], 'dist': mc[3][p_i], 'los': mc[4][p_i], 'pl': mc[5][p_i], 'tr_nlos': mc[6][p_i], 'los a prori': mc[7][p_i], **common_d})\n",
    "            p = rand_points[p_i]\n",
    "            b_p = targets[tgt][p[0], p[1]]\n",
    "            tr_nlos.append(mc[6][p_i])\n",
    "            los.append(mc[4][p_i])\n",
    "            if mc[0][p_i]:\n",
    "                base_points.append(b_p)\n",
    "                speed.append(mc[0][p_i])\n",
    "            if mc[1][p_i]:\n",
    "                base_points_los.append(b_p)\n",
    "                speed_los.append(mc[1][p_i])\n",
    "            if mc[2][p_i]:\n",
    "                base_points_nlos.append(b_p)\n",
    "                speed_nlos.append(mc[2][p_i])\n",
    "\n",
    "        #compute weighted average + CI and save in a dataframe\n",
    "        res.append({'type': 'los and nlos', \n",
    "                    'target': tgt, \n",
    "                    'tr_nlos': sum(tr_nlos)/np.count_nonzero(los),\n",
    "                    'speed_m': np.average(speed, weights=base_points), \n",
    "                    'speed_ci': conf_int(speed, weights=base_points), \n",
    "                    'speed_std': std(speed, weights=base_points), \n",
    "                    'speed_varerr': std(speed, weights=base_points)/np.average(speed, weights=base_points), \n",
    "                    **common_d})\n",
    "        res.append({'type': 'onlynlos', \n",
    "                    'target': tgt,\n",
    "                    'speed_m': np.average(speed_nlos, weights=base_points_nlos), \n",
    "                    'speed_ci': conf_int(speed_nlos, weights=base_points_nlos),\n",
    "                    'speed_std': std(speed_nlos, weights=base_points_nlos),\n",
    "                    **common_d})\n",
    "        res.append({'type': 'onlylos', \n",
    "                    'target': tgt,\n",
    "                    'speed_m': np.average(speed_los, weights=base_points_los), \n",
    "                    'speed_ci': conf_int(speed_los, weights=base_points_los),\n",
    "                    'speed_std': std(speed_los, weights=base_points_los),\n",
    "                    **common_d})\n",
    "res_df = pd.DataFrame(res)\n",
    "res_un_df = pd.DataFrame(res_un)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate figure for capacity LoS vs NLoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = res_df[(res_df.target.isin(['sidewalkparks']) & res_df.base.isin(['sidewalkparks'])) | (res_df.target.isin(['cars_sumo_nonorm']) & res_df.base.isin(['cars_sumo_nonorm']))].groupby(['dens', 'target', 'type']).speed_m.last().reset_index().pivot('dens', ['target', 'type'])\n",
    "\n",
    "output_df = output_df/1000\n",
    "output_df.columns = ['cars_losnlos', 'cars_los', 'cars_nlos', 'parks_losnlos', 'parks_los', 'parks_nlos']\n",
    "output_df.to_csv('../processed_results/capacity_losnlos.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure for NLoS links choosen even if there was a LoS one available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "myres = res_un_df[(res_un_df.base.isin(['cars_sumo_nonorm']) & res_un_df.target.isin(['cars_sumo_nonorm'])) | (res_un_df.base.isin(['sidewalkparks']) & res_un_df.target.isin(['sidewalkparks']))]\n",
    "\n",
    "output_df = myres.groupby(['dens', 'target']).tr_nlos.mean().reset_index().pivot('dens', 'target', 'tr_nlos')\n",
    "output_df.columns = ['cars', 'parks']\n",
    "output_df.to_csv('../processed_results/tr_nlos.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ECDF of the number links wrt capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.20000000e-01, 2.40000000e-01, 3.60000000e-01, ...,\n",
       "        1.17600000e+01, 1.18800000e+01, 1.20000000e+01],\n",
       "       [4.86940676e-01, 7.04393602e-01, 8.02996558e-01, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "       ...,\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "       [0.00000000e+00, 2.83386468e-03, 2.01912859e-02, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def empirical_cdf(s: pd.Series, bins):\n",
    "    # Sort the data into `n_bins` evenly spaced bins:\n",
    "    discretized = pd.cut(s, bins)\n",
    "    # Count the number of datapoints in each bin:\n",
    "    bin_counts = discretized.value_counts().sort_index().reset_index()\n",
    "    # Calculate the locations of each bin as just the mean of the bin start and end:\n",
    "    bin_counts[\"loc\"] = (pd.IntervalIndex(bin_counts[\"index\"]).left + pd.IntervalIndex(bin_counts[\"index\"]).right) / 2\n",
    "    # Compute the CDF with cumsum:\n",
    "    return bin_counts.set_index(\"loc\").iloc[:, -1].cumsum()\n",
    "\n",
    "\n",
    "data = res_un_df[(res_un_df.target.isin(['cars_sumo_nonorm']) & res_un_df.base.isin(['cars_sumo_nonorm'])) | (res_un_df.target.isin(['sidewalkparks']) & res_un_df.base.isin(['sidewalkparks'])) & \n",
    "                       res_un_df.dens.isin([5,15,35,45])]\n",
    "\n",
    "bins =  np.linspace(0,12000, 101)\n",
    "res = np.zeros(shape=(17,100))\n",
    "res[0,:] = bins[1:]/1000 #to gbps\n",
    "i=1\n",
    "head = ['capacity']\n",
    "for target in ['cars_sumo_nonorm', 'sidewalkparks']:\n",
    "    for dens in [5,15,35,45]:\n",
    "        for los in [0,1]:\n",
    "            if los:\n",
    "                head.append(f'{target}_{dens}_los')\n",
    "            else:\n",
    "                head.append(f'{target}_{dens}_nlos')\n",
    "            ecdf = empirical_cdf(data[(data.los==los) &(data.dens==dens) & (data.target == target)].speed, bins)\n",
    "            res[i,:] = ecdf / max(ecdf)\n",
    "            i+=1\n",
    "\n",
    "\n",
    "np.savetxt('../processed_results/ecdf_capacity.csv', res.T, header=', '.join(head), delimiter=',')\n",
    "\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "myres= res_df[(res_df.type.isin(['los and nlos'])) & (res_df.target.isin(['cars_sumo_nonorm']) & res_df.base.isin(['cars_sumo_nonorm'])) | (res_df.target.isin(['sidewalkparks']) & res_df.base.isin(['sidewalkparks']))]\n",
    "\n",
    "output_df = myres.groupby([\"target\", \"dens\"]).mean().speed_varerr.reset_index().pivot(\"dens\",\"target\", \"speed_varerr\")\n",
    "output_df.to_csv('../processed_results/speed_varerr.csv')\n",
    "\n",
    "#sns.relplot(data=myres, x='dens', y='speed_varerr', hue='base', style='target', kind='line')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "truenets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
